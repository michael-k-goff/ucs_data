# Existential risk

import helper

risk_table = [
    ["<b>Risk Factor</b>","<b>Current Damages</b>","<b>Worst Case Scenario</b>","<b>Risk</b>","<b>Mitigation</b>"],
    ["Climate Change","Estimated 250,000 annual deaths in 2030 (WHO). At a social cost of carbon of $50/ton, current annual emissions cause $2 trillion of long term damage.","Catastrophic risk","Ongoing warming and damages are certain. The worst mainstream projection is a loss of 22-37% under RCP8.5 (Burke). Worse is possible under extreme outcomes, such as release of methane hydrates, but the likelihood of such events is unknown and they will unfold over centuries.","Deploy low carbon energy, energy efficiency, carbon removal, adaptation to changing climate, geoengineering."],
    ["Artificial Intelligence Catastrophe","Cybercrime caused an estimated $3 trillion in damages in 2015 and is projected to reach $6 trillion in 2021 (Herjavec Group)","Human extinction","Not imminent. Turchin and Denkenberger do not estimate the likelihood of an AI catastrophe but regard it as humanity's most serious existential risk.","Cybersecurity, improved software engineering practices, research into AI safety."],
    ["Pandemic","Infectious diseases kill over 8 million people per year (GBDCN). The 2020 coronavirus pandemic has killed over 120,000 people and caused tens of trillions of dollars of damage (update as needed).","Existential risk","Annual risk of 1.6*10<sup>-8</sup> to 8*10<sup>-7</sup> from lab escape, 1.4*10<sup>-6</sup> from terrorism, 5*10<sup>-7</sup> from war (Millett and Snyder-Beattie).","Disease surveillance, capacity for testing, public health (hand washing, social distance, quarantine, etc.), vaccine and antiviral development, reserves of medical equipment, plan to insure continuity of essential services (WHO), regulation of hazards such as biolabs and wet markets."],
    ["War","War and terrorism killed about 130,000 people in 2017 (GBDCN). World military spending was about $1.8 trillion in 2018 (SIPRI). The deadliest war in history, World War II, killed over 70 million people over six years, including deaths from war-related famine and disease and crimes against humanity.","Catastrophic risk","Annual risk of nuclear war of 0.01 (Lundgren). Worst case scenario and potential of existential risk depend on highly uncertain factors, such as prospect of nuclear winter (Scouras).","Arms control, international diplomacy"],
    ["Supervolcano","Earthquakes and volcanoes caused on average 27000 deaths and $37 billion of damages per year in the 2010s. The majority of deaths and damages occurred in the 2010 Haiti earthquake and the 2011 Tohoku earthquake and tsunami respectively (EM-DAT).","Human Extinction","Annual risk of eruption 10<sup>-5</sup>, but an eruption would not necessarily end civilization (Turchin and Denkenberger).","Engineering responses such as drilling to release heat (Wilcox et al.)."],
    ["Asteroid Impact","Since 1900, the only asteroid that has done significant damage to humans was the 2013 Chelyabinsk impact, which caused 1600 injuries (Talbert) and $33 million in damages (EM-DAT).","Human extinction","Annual risk of 10<sup>-8</sup>. Larger risk of lesser impacts.","Detection and Deflection (NASA)",""],
    ["Nanotechnology Catastrophe","None yet","Human extinction","Annual risk of destruction of life on Earth from \"gray goo\" catastrophe estimated at 10<sup>-4</sup> (1% in 21st century) (Turchin and Denkenberger). Additional risk that molecular nanotechnology can be used to manufacture dangerous weapons. Such risks require substantial and speculative technological advancement.","Depends on how technology develops."],
    ["Physics Catastrophe","None","Destruction of Earth or cosmos","Highly uncertain. Annual risk of at most 2*10<sup>-7</sup> for strangelet catastrophe estimated from the Relativistic Heavy Ion Collider (Dar ab et al.). Risks of catastrophes such as strangelet release, black hole creation, or vacuum collapse generally regarded as implausible (Ellis et al.).","Monitor novel experiments for potential risk."]
]

helper.save_image({
    "filename":"existential_risk.jpg",
    "status":"Done",
    "details":"John is interested in a plot that compares existential risk, or risks that could cause human extinction or permanently prevent industrial civilization from existing. This will help put climate change in context of other things to worry about. We could get creative and add all sorts of other risks to this list, but I am trying to keep it to scenarios that are either plausible or occupy public attention. As for ordering, I put climate change at the top because that is our greatest concern at Urban Cruise Ship, and for the rest I arranged them from most to least serious, based on my subjective estimate. For the Worst Case Scenario column, 'catastrophic risk' means that a large fraction of humanity could die, but it does not plausibly threaten civilization as a whole. 'Existential risk' means that it threatens civilization as a whole, but does not threaten human extinction. Since we are dealing with low probability events, what is plausible is admittedly somewhat subjective.",
    "table":risk_table,
    "references":["bio_xrisk","disease_burden","asteroid","who_pandemic","emdat","talbert","supervolcano","turchin","scouras","sipri","who_cc","burke","strangelet","lhc_safety","herjavec"],
    "source_file":"risk.py"
})